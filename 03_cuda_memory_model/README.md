# cuda内存模型

## 流多处理器SM

![流多处理器01](./images/流多处理器01.png)
![流多处理器02](./images/流多处理器02.png)

GPU中每个SM都能支持数百个线程并发执行，每个GPU通常有多个SM,当一个核函数的网格被启动的时候，多个block会被同时分配给可用的
SM上执行

线程束中的线程是并行的，在某时刻T,SM上只执行一个线程束，也就是32个线程在同时同步执行，线程束中的每个线程执行同一个指令，包括有分支的部分。

在Fermi架构中，每个SM有两个线程束调度器，和两个指令调度单元，当一个线程块被指定给一个SM时，线程块内的所有线程被分成线程束，两个线程束选择其中两个线程束，在用指令调度器存储两个线程束要执行的指令。当某个线程块被分配到一个SM上的时候，会被分成多个线程束，线程束在SM上交替执行：
![线程束并发执行01](./images/线程束并发执行01.png)

## SIMD vs SIMT
> 单指令多数据SIMD vs 单指令多线程SIMT
1. 单指令多数据: 单指令多数据的执行属于向量机，比如我们有四个数字要加上四个数字，那么我们可以用这种单指令多数据的指令来一次完成本来要做四次的运算。这种机制的问题就是过于死板，不允许每个分支有不同的操作，所有分支必须同时执行相同的指令，必须执行没有例外。
2. 单指令多线程: 相比之下单指令多线程SIMT就更加灵活了，虽然两者都是将相同指令广播给多个执行单元，但是SIMT的某些线程可以选择不执行
也就是说同一时刻所有线程被分配给相同的指令，SIMD规定所有人必须执行，而SIMT则规定有些人可以根据需要不执行，这样SIMT就保证了线程级别的并行，而SIMD更像是指令级别的并行。
SIMT包括以下SIMD不具有的关键特性：

    + 每个线程都有自己的指令地址计数器
    + 每个县城都有自己的寄存器状态
    + 每个线程可以有一个独立的执行路径

而上面这三个特性在编程模型可用的方式就是给每个线程一个唯一的标号（blckIdx,threadIdx），并且这三个特性保证了各线程之间的独立。

从概念上讲，32是SM以SIMD方式同时处理的工作粒度，一个SM上在某一个时刻，有32个线程在执行同一条指令，这32个线程可以选择性执行，虽然有些可以不执行，但是他也不能执行别的指令，需要另外需要执行这条指令的线程执行完，然后再继续下一条

## 线程模型与物理模型结构

![线程模型与物理模型结构](./images/线程模型与物理模型.png)

因为SM有限，虽然我们的编程模型层面看所有线程都是并行执行的，但是在微观上看，所有线程块也是分批次的在物理层面的机器上执行，线程块里不同的线程可能进度都不一样，但是同一个线程束内的线程拥有相同的进度。
并行就会引起竞争，多线程以未定义的顺序访问同一个数据，就导致了不可预测的行为，CUDA只提供了一种块内同步的方式，块之间没办法同步！

同一个SM上可以有不止一个常驻的线程束，有些在执行，有些在等待，他们之间状态的转换是不需要开销的。

## 线程束
![线程束01](./images/线程束01.png)
![线程束02](./images/线程束02.png)

## 线程束分化
> 具体代码分析见: 
> 1.(单个分支结构放到同一个线程束中)thread_warp/thread_warp_demo01
> 2.(避免分支分化_规约问题) thread_warp/thread_warp_demo02

+ [参考链接: 避免分支分化](https://face2ai.com/CUDA-F-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96/)

假设下面这段代码是核函数的一部分，那么当一个线程束的32个线程执行这段代码的时候，如果其中16个执行if中的代码段，而另外16个执行else中的代码块，同一个线程束中的线程，执行不同的指令，这叫做线程束的分化。
```
if (con)
{
    //do something
}
else
{
    //do something
}
```
+ 优化方向: 避免同一个线程束内的线程分化，而让我们能控制线程束内线程行为的原因是线程块中线程分配到线程束是有规律的而不是随机的。这就使得我们根据线程编号来设计分支是可以的，补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。

线程束内的线程是可以被我们控制的，那么我们就把都执行if的线程塞到一个线程束中，或者让一个线程束中的线程都执行if，另外线程都执行else的这种方式可以将效率提高很多。

## 内存结构层次
> 查看gpu内存使用情况
> `nvcc --resource-usage hello.cu -o hello -arch=sm_61`  sm_61(计算能力)

![内存结构层次01](./images/内存结构层次01.png)

## cuda内存模型
![cuda内存模型01](./images/cuda内存模型01.png)
![cuda内存模型02](./images/cuda内存模型02.png)

## 寄存器
![寄存器01](./images/寄存器01.png)
![寄存器02](./images/寄存器02.png)
![寄存器溢出](./images/寄存器溢出01.png)

## 本地内存
![本地内存01](./images/本地内存01.png)
![本地内存02](./images/本地内存02.png)

## 共享内存
![共享内存01](./images/共享内存01.png)
![共享内存02](./images/共享内存02.png)
![共享内存03](./images/共享内存03.png)
![静态共享内存](./images/静态共享内存01.png)
![共享内存与一级缓存划分](./images/共享内存与一级缓存划分.png)

1. 在定义共享内存的时候，一般需要定义长度等于线程块大小的数组
2. 核函数里面的第三个参数就是指定的动态共享内存的大小`kernel<<<grid, block, 32>>>();`

## 全局内存
![全局内存01](./images/全局内存01.png)
![全局内存02](./images/全局内存02.png)

1. 静态全局内存的数量是在编译期就确定下来的，需要在所有的主机和设备外面进行定义声明。
2. 在核函数里面可以直接对静态全局内存进行访问。
3. 主机函数不能直接访问静态全局变量，通过cudaMemcpyToSymbol, cudaMemcpyFromSymbol进行通信

## 常量内存
![常量内存01](./images/常量内存01.png)
![常量内存02](./images/常量内存02.png)

1. 常量内存必须定义在核函数/主机函数之外
2. `__global__ void kernel(int N)`  // 这里的N就是存放在常量内存中的


